{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Exploring The Data\n",
    "We will use the same data set when we built a Logistic Regression in Python, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (Yes/No) to a term deposit. The dataset can be downloaded from Kaggle.<h4>\n",
    "    \n",
    "    ref:https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "df = spark.read.csv('bank.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.18.1.zip (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: numpy\n",
      "  Building wheel for numpy (PEP 517) ... \u001b[?25l|"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data for Machine Learning\n",
    "The process includes Category Indexing, One-Hot Encoding and VectorAssembler — a feature transformer that merges multiple columns into a vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split data into train and test sets, and set seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 7764\n",
      "Test Dataset Count: 3398\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "|age|        job|label|       rawPrediction|prediction|         probability|\n",
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "| 18|    student|  1.0|[-0.2251430600501...|       1.0|[0.44395079358481...|\n",
      "| 18|    student|  1.0|[-0.5636188230682...|       1.0|[0.36271054806663...|\n",
      "| 19|    student|  1.0|[-0.5684347598631...|       1.0|[0.36159807423288...|\n",
      "| 19|    student|  1.0|[-3.4591930495953...|       1.0|[0.03049588242942...|\n",
      "| 20|blue-collar|  1.0|[-0.9641237569114...|       1.0|[0.27605331018526...|\n",
      "| 20|    student|  1.0|[-0.3057798595901...|       1.0|[0.42414516007933...|\n",
      "| 20|    student|  1.0|[-3.3025718642084...|       1.0|[0.03548306451732...|\n",
      "| 20|    student|  0.0|[1.46112621265899...|       0.0|[0.81170486564410...|\n",
      "| 20|    student|  1.0|[-0.8211329030062...|       1.0|[0.30552322896115...|\n",
      "| 20|    student|  1.0|[-0.8685589128467...|       1.0|[0.29555425071253...|\n",
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.8872231023652682\n"
     ]
    }
   ],
   "source": [
    "#Evaluate our Logistic Regression model.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier\n",
    "Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+---------------+----------+--------------------+\n",
      "|age|        job|label|  rawPrediction|prediction|         probability|\n",
      "+---+-----------+-----+---------------+----------+--------------------+\n",
      "| 18|    student|  1.0|[3327.0,1063.0]|       0.0|[0.75785876993166...|\n",
      "| 18|    student|  1.0|[3327.0,1063.0]|       0.0|[0.75785876993166...|\n",
      "| 19|    student|  1.0|[3327.0,1063.0]|       0.0|[0.75785876993166...|\n",
      "| 19|    student|  1.0|   [39.0,412.0]|       1.0|[0.08647450110864...|\n",
      "| 20|blue-collar|  1.0| [202.0,1254.0]|       1.0|[0.13873626373626...|\n",
      "| 20|    student|  1.0|[3327.0,1063.0]|       0.0|[0.75785876993166...|\n",
      "| 20|    student|  1.0|   [39.0,412.0]|       1.0|[0.08647450110864...|\n",
      "| 20|    student|  0.0|[3327.0,1063.0]|       0.0|[0.75785876993166...|\n",
      "| 20|    student|  1.0|[3327.0,1063.0]|       0.0|[0.75785876993166...|\n",
      "| 20|    student|  1.0|[3327.0,1063.0]|       0.0|[0.75785876993166...|\n",
      "+---+-----------+-----+---------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.5093844673281636\n"
     ]
    }
   ],
   "source": [
    "#Evaluate our Decision Tree model.\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "|age|        job|label|       rawPrediction|prediction|         probability|\n",
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "| 18|    student|  1.0|[9.65052898295371...|       1.0|[0.48252644914768...|\n",
      "| 18|    student|  1.0|[8.3630953852178,...|       1.0|[0.41815476926089...|\n",
      "| 19|    student|  1.0|[7.85517548687664...|       1.0|[0.39275877434383...|\n",
      "| 19|    student|  1.0|[2.25149808487983...|       1.0|[0.11257490424399...|\n",
      "| 20|blue-collar|  1.0|[5.79108197526126...|       1.0|[0.28955409876306...|\n",
      "| 20|    student|  1.0|[8.93586153317299...|       1.0|[0.44679307665864...|\n",
      "| 20|    student|  1.0|[2.07923152536544...|       1.0|[0.10396157626827...|\n",
      "| 20|    student|  0.0|[15.5660186353400...|       0.0|[0.77830093176700...|\n",
      "| 20|    student|  1.0|[7.70258418113884...|       1.0|[0.38512920905694...|\n",
      "| 20|    student|  1.0|[7.70258418113884...|       1.0|[0.38512920905694...|\n",
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8814336140526394\n"
     ]
    }
   ],
   "source": [
    "#Evaluate our Random Forest Classifier.\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "|age|        job|label|       rawPrediction|prediction|         probability|\n",
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "| 18|    student|  1.0|[-0.1733144470155...|       1.0|[0.41420014349658...|\n",
      "| 18|    student|  1.0|[-0.1698137109977...|       1.0|[0.41589998355429...|\n",
      "| 19|    student|  1.0|[-0.3894578956498...|       1.0|[0.31455360488836...|\n",
      "| 19|    student|  1.0|[-1.1260719899887...|       1.0|[0.09516468996793...|\n",
      "| 20|blue-collar|  1.0|[-0.9981556330144...|       1.0|[0.11959075976740...|\n",
      "| 20|    student|  1.0|[-0.3199910621568...|       1.0|[0.34525058022815...|\n",
      "| 20|    student|  1.0|[-1.1120132532229...|       1.0|[0.09761355378069...|\n",
      "| 20|    student|  0.0|[1.12703537577411...|       0.0|[0.90500109182949...|\n",
      "| 20|    student|  1.0|[-0.6364679740189...|       1.0|[0.21875508047278...|\n",
      "| 20|    student|  1.0|[-0.6364679740189...|       1.0|[0.21875508047278...|\n",
      "+---+-----------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gradient-Boosted Tree Classifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8909304733316615\n"
     ]
    }
   ],
   "source": [
    "#Evaluate our Gradient-Boosted Tree Classifier.\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: all)\n",
      "featuresCol: features column name. (default: features)\n",
      "labelCol: label column name. (default: label)\n",
      "lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic (default: logistic)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxIter: max number of iterations (>= 0). (default: 20, current: 10)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 3504127614838123891)\n",
      "stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(gbt.explainParams())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
