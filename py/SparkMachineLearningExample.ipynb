{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression using spark MLlib<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dataset is the <strong>Pima Indians Diabetes Dataset</strong> [https://www.kaggle.com/uciml/pima-indians-diabetes-database/version/1]. \n",
    "This dataset belongs to National Institute of Diabetes and Digestive and Kidney Diseases. The dataset contains data used to classifiy if\n",
    "someone has diabetes or not. The dataset contains felmale person's data. Features include: number of pregnancies, Glucose concenteration, Blood pressure, skinThickness, \n",
    "Insulin, BMI, DiabetesPedigreeFunction,Age. The label is 1 for diabetic and zero for non diabetic. 268 Participants out of 768 are 1(diabetec). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession #import spark session\n",
    "spark= SparkSession.builder.appName(\"MLlib demo\").getOrCreate() #Create a spark session using MLlib demo as name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Diabetes Datast stored</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load diabetes.csv from loca storage to a data frame using spark, header=True <The first row is column header>, inferSchema=True, \n",
    "#option to inferSchema directly from the dataset\n",
    "diab_ds=spark.read.csv('diabetes.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Get the name of features of the dataset</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pregnancies',\n",
       " 'Glucose',\n",
       " 'BloodPressure',\n",
       " 'SkinThickness',\n",
       " 'Insulin',\n",
       " 'BMI',\n",
       " 'DiabetesPedigreeFunction',\n",
       " 'Age',\n",
       " 'Outcome']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_ds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Get the datatype of the features</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pregnancies', 'int'),\n",
       " ('Glucose', 'int'),\n",
       " ('BloodPressure', 'int'),\n",
       " ('SkinThickness', 'int'),\n",
       " ('Insulin', 'int'),\n",
       " ('BMI', 'double'),\n",
       " ('DiabetesPedigreeFunction', 'double'),\n",
       " ('Age', 'int'),\n",
       " ('Outcome', 'int')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_ds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Read First row of dataset</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Pregnancies=6, Glucose=148, BloodPressure=72, SkinThickness=35, Insulin=0, BMI=33.6, DiabetesPedigreeFunction=0.627, Age=50, Outcome=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_ds.head() #returns the first row as key and value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <strong>In order to see the number of examples for each class, we use groupBy outcome and apply count operation. \n",
    "The result shows that we have 268 diabetic and 500 non diabetic examples. This shows that this dataset is unbalanced. </strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Outcome|count|\n",
      "+-------+-----+\n",
      "|      1|  268|\n",
      "|      0|  500|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diab_ds.groupBy('Outcome').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset preparataion and preprocessing for Logistic regression </h3>\n",
    "\n",
    "<p>We need <strong>Vector Assembler </strong> which is a transformer that combines a set of selected features in to a\n",
    "a single feature vector. For example for our dataset with 8 features, it will combine these 8 features in to one \n",
    "feature vector. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.18.1.zip (5.4 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: numpy\n",
      "  Building wheel for numpy (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for numpy: filename=numpy-1.18.1-cp36-cp36m-linux_x86_64.whl size=13382120 sha256=8de28aa28fe356f8108fd4d1954b15d93d1f4f19ef7836c42517d0ff4af7d3d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/18/20/83339b2576b5911519a6b616d8b4a6df8b14358ba5cd612a0b\n",
      "Successfully built numpy\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.18.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#import vectors and vector assembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vectorAssembler transformer that takes all features and maps them in to one vector called <features>\n",
    "assembler= VectorAssembler(\n",
    "inputCols=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction',\n",
    "           'Age'],\n",
    "    outputCol=\"features\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_ds_vec=assembler.transform(diab_ds) #Transform diab_db using assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
      "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|            features|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
      "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|[6.0,148.0,72.0,3...|\n",
      "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|[1.0,85.0,66.0,29...|\n",
      "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|[8.0,183.0,64.0,0...|\n",
      "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diab_ds_vec.show(3) #show the first element "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Now we select features and outcome to build our dataset ready for preprocessing </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|Outcome|\n",
      "+--------------------+-------+\n",
      "|[6.0,148.0,72.0,3...|      1|\n",
      "|[1.0,85.0,66.0,29...|      0|\n",
      "|[8.0,183.0,64.0,0...|      1|\n",
      "+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diab_db_final= diab_ds_vec.select('features','Outcome') #select features and Outcome from diab_ds_vec\n",
    "diab_db_final.show(3) #show first three elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Feature scaling</strong>\n",
    "<p>Since the values of each feture are in different scale, we apply scaling to put all featues on the same scale. We use <strong>Standard Scaler</strong> which is MLlib transformer that transformes a dataset of vector rows by scaling each feature to have a zero mean or unit standard devation. Feature scaling can imporve accuracy for some classifiers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#withStd converts features to unit standard deviation and withMean: centers the data withMean before scaling\n",
    "scaler=StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now build scaler model using diab_db_final\n",
    "ScalerModel=scaler.fit(diab_db_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+\n",
      "|            features|Outcome|      scaledFeatures|\n",
      "+--------------------+-------+--------------------+\n",
      "|[6.0,148.0,72.0,3...|      1|[1.78063837321943...|\n",
      "+--------------------+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Transform diab_db_final using the scaler model\n",
    "diab_db_scaled= ScalerModel.transform(diab_db_final)\n",
    "diab_db_scaled.show(1) #show first row of scaled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Now we split our scaled dataset in to training and testing data. We use 75% for training and 25% for testing</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|      scaledFeatures|Outcome|\n",
      "+--------------------+-------+\n",
      "|(8,[0,1,6,7],[0.5...|      0|\n",
      "|(8,[0,1,6,7],[0.5...|      0|\n",
      "|(8,[0,1,6,7],[0.8...|      0|\n",
      "+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diab_db_train, diab_db_test=diab_db_scaled.select('scaledFeatures','Outcome').randomSplit([0.75,0.25])\n",
    "diab_db_train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Logistic Regression classifer</strong>\n",
    "<p>Logistic regression is used to solve binary classification problem. Binomial logistic regeression is used for binary classification and multnomial is used for multi-class classification</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lrModel= LogisticRegression(maxIter=50, featuresCol='scaledFeatures', labelCol='Outcome') #build a model by specifying the labelCol as Outcome\n",
    "lrModel=lrModel.fit(diab_db_train) # Train the model using diab_db_train\n",
    "trainingSummary= lrModel.summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Evaluation of logistic regression using test data</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|      scaledFeatures|Outcome|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|(8,[0,1,6,7],[1.7...|      0|[2.88688855254209...|[0.94719447233909...|       0.0|\n",
      "|(8,[1,5,6,7],[5.2...|      1|[-1.3156757608887...|[0.21153863383538...|       1.0|\n",
      "|[0.0,2.9087389538...|      0|[2.09266489629441...|[0.89018819869731...|       0.0|\n",
      "|[0.0,2.9400157167...|      0|[1.62563943834837...|[0.83557140956014...|       0.0|\n",
      "|[0.0,2.9712924797...|      1|[2.19498630430049...|[0.89979837499423...|       0.0|\n",
      "|[0.0,3.0338460056...|      0|[1.74460304716309...|[0.85127079468984...|       0.0|\n",
      "|[0.0,3.0651227685...|      0|[3.06592745158107...|[0.95546520065146...|       0.0|\n",
      "|[0.0,3.1276762944...|      0|[2.14000371221546...|[0.89473096012218...|       0.0|\n",
      "|[0.0,3.1902298203...|      0|[2.22243331431568...|[0.90224601966007...|       0.0|\n",
      "|[0.0,3.1902298203...|      0|[2.26914876909187...|[0.90628951871808...|       0.0|\n",
      "|[0.0,3.2527833462...|      0|[2.93499094799017...|[0.94954930569349...|       0.0|\n",
      "|[0.0,3.2840601091...|      0|[3.07710291624539...|[0.95593831977146...|       0.0|\n",
      "|[0.0,3.4717206868...|      0|[1.98973617166049...|[0.87971522297711...|       0.0|\n",
      "|[0.0,3.6906580274...|      1|[0.66998269529723...|[0.66149928437967...|       0.0|\n",
      "|[0.0,3.7219347903...|      0|[1.04422347940590...|[0.73966410682761...|       0.0|\n",
      "|[0.0,3.7532115533...|      0|[1.77221068328091...|[0.85473237573530...|       0.0|\n",
      "|[0.0,3.7844883162...|      1|[1.52326597902825...|[0.82101890922947...|       0.0|\n",
      "|[0.0,3.9095953680...|      0|[2.01755295267792...|[0.88262774204530...|       0.0|\n",
      "|[0.0,3.9095953680...|      0|[2.41111896648109...|[0.91767126011335...|       0.0|\n",
      "|[0.0,3.9721488939...|      0|[0.88393901648757...|[0.70763781658620...|       0.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator #We use this to evaluate all binary classifications\n",
    "predictions= lrModel.evaluate(diab_db_test) #apply classification on test data\n",
    "predictions.predictions.show()\n",
    "#evaluator= BinaryClassificationEvaluator(rawPredictionCol='predection', labelCol=\"Outcome\")\n",
    "#evaluator.evaluate(predections.predection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
