{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression using spark MLlib<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dataset is the <strong>Pima Indians Diabetes Dataset</strong> [https://www.kaggle.com/uciml/pima-indians-diabetes-database/version/1]. \n",
    "This dataset belongs to National Institute of Diabetes and Digestive and Kidney Diseases. The dataset contains data used to classifiy if\n",
    "someone has diabetes or not. The dataset contains felmale person's data. Features include: number of pregnancies, Glucose concenteration, Blood pressure, skinThickness, \n",
    "Insulin, BMI, DiabetesPedigreeFunction,Age. The label is 1 for diabetic and zero for non diabetic. 268 Participants out of 768 are 1(diabetec). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession #import spark session\n",
    "spark= SparkSession.builder.appName(\"MLlib demo\").getOrCreate() #Create a spark session using MLlib demo as name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Diabetes Datast stored</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load diabetes.csv from loca storage to a data frame using spark, header=True <The first row is column header>, inferSchema=True, \n",
    "#option to inferSchema directly from the dataset\n",
    "diab_ds=spark.read.csv('diabetes.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Get the name of features of the dataset</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_ds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Get the datatype of the features</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_ds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Read First row of dataset</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_ds.head() #returns the first row as key and value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <strong>In order to see the number of examples for each class, we use groupBy outcome and apply count operation. \n",
    "The result shows that we have 268 diabetic and 500 non diabetic examples. This shows that this dataset is unbalanced. </strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_ds.groupBy('Outcome').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset preparataion and preprocessing for Logistic regression </h3>\n",
    "\n",
    "<p>We need <strong>Vector Assembler </strong> which is a transformer that combines a set of selected features in to a\n",
    "a single feature vector. For example for our dataset with 8 features, it will combine these 8 features in to one \n",
    "feature vector. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import vectors and vector assembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vectorAssembler transformer that takes all features and maps them in to one vector called <features>\n",
    "assembler= VectorAssembler(\n",
    "inputCols=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction',\n",
    "           'Age'],\n",
    "    outputCol=\"features\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_ds_vec=assembler.transform(diab_ds) #Transform diab_db using assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_ds_vec.show(3) #show the first element "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Now we select features and outcome to build our dataset ready for preprocessing </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_db_final= diab_ds_vec.select('features','Outcome') #select features and Outcome from diab_ds_vec\n",
    "diab_db_final.show(3) #show first three elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Feature scaling</strong>\n",
    "<p>Since the values of each feture are in different scale, we apply scaling to put all featues on the same scale. We use <strong>Standard Scaler</strong> which is MLlib transformer that transformes a dataset of vector rows by scaling each feature to have a zero mean or unit standard devation. Feature scaling can imporve accuracy for some classifiers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#withStd converts features to unit standard deviation and withMean: centers the data withMean before scaling\n",
    "scaler=StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now build scaler model using diab_db_final\n",
    "ScalerModel=scaler.fit(diab_db_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform diab_db_final using the scaler model\n",
    "diab_db_scaled= ScalerModel.transform(diab_db_final)\n",
    "diab_db_scaled.show(1) #show first row of scaled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Now we split our scaled dataset in to training and testing data. We use 75% for training and 25% for testing</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_db_train, diab_db_test=diab_db_scaled.select('scaledFeatures','Outcome').randomSplit([0.75,0.25])\n",
    "diab_db_train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Logistic Regression classifer</strong>\n",
    "<p>Logistic regression is used to solve binary classification problem. Binomial logistic regeression is used for binary classification and multnomial is used for multi-class classification</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lrModel= LogisticRegression(maxIter=50, featuresCol='scaledFeatures', labelCol='Outcome') #build a model by specifying the labelCol as Outcome\n",
    "lrModel=lrModel.fit(diab_db_train) # Train the model using diab_db_train\n",
    "trainingSummary= lrModel.summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Evaluation of logistic regression using test data</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator #We use this to evaluate all binary classifications\n",
    "predictions= lrModel.evaluate(diab_db_test) #apply classification on test data\n",
    "predictions.predictions.show()\n",
    "#evaluator= BinaryClassificationEvaluator(rawPredictionCol='predection', labelCol=\"Outcome\")\n",
    "#evaluator.evaluate(predections.predection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
